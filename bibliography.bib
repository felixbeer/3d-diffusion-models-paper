
@inproceedings{liu_one-2-3-45_2023,
  title     = {One-2-3-45: {Any} {Single} {Image} to {3D} {Mesh} in 45 {Seconds} without {Per}-{Shape} {Optimization}},
  abstract  = {Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360degree meshes. Without costly optimizations, our method reconstructs 3D shapes in significantly less time than existing methods. Moreover, our method favors better geometry, generates more 3D consistent results, and adheres more closely to the input image. We evaluate our approach on both synthetic data and in-the-wild images and demonstrate its superiority in terms of both mesh quality and runtime. In addition, our approach can seamlessly support the text-to-3D task by integrating with off-the-shelf text-to-image diffusion models.},
  language  = {en},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 36},
  author    = {Liu, Minghua and Xu, Chao and Jin, Haian and Chen, Linghao and Varma T, Mukund and Xu, Zexiang and Su, Hao},
  month     = jun,
  year      = {2023},
  file      = {Liu et al. - One-2-3-45 Any Single Image to 3D Mesh in 45 Seco.pdf:/Users/felix/Zotero/storage/UUVV6DEK/Liu et al. - One-2-3-45 Any Single Image to 3D Mesh in 45 Seco.pdf:application/pdf}
}

@misc{liu_one-2-3-45_2023-1,
  title      = {One-2-3-45++: {Fast} {Single} {Image} to {3D} {Objects} with {Consistent} {Multi}-{View} {Generation} and {3D} {Diffusion}},
  shorttitle = {One-2-3-45++},
  url        = {http://arxiv.org/abs/2311.07885},
  abstract   = {Recent advancements in open-world 3D object generation have been remarkable, with image-to-3D methods offering superior fine-grained control over their text-to-3D counterparts. However, most existing models fall short in simultaneously providing rapid generation speeds and high fidelity to input images - two features essential for practical applications. In this paper, we present One-2-3-45++, an innovative method that transforms a single image into a detailed 3D textured mesh in approximately one minute. Our approach aims to fully harness the extensive knowledge embedded in 2D diffusion models and priors from valuable yet limited 3D data. This is achieved by initially finetuning a 2D diffusion model for consistent multi-view image generation, followed by elevating these images to 3D with the aid of multi-view conditioned 3D native diffusion models. Extensive experimental evaluations demonstrate that our method can produce high-quality, diverse 3D assets that closely mirror the original input image. Our project webpage: https://sudo-ai-3d.github.io/One2345plus\_page.},
  urldate    = {2024-03-18},
  publisher  = {arXiv},
  author     = {Liu, Minghua and Shi, Ruoxi and Chen, Linghao and Zhang, Zhuoyang and Xu, Chao and Wei, Xinyue and Chen, Hansheng and Zeng, Chong and Gu, Jiayuan and Su, Hao},
  month      = nov,
  year       = {2023},
  note       = {arXiv:2311.07885 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
  file       = {arXiv.org Snapshot:/Users/felix/Zotero/storage/P33PW48P/2311.html:text/html;Full Text PDF:/Users/felix/Zotero/storage/ZVULD83E/Liu et al. - 2023 - One-2-3-45++ Fast Single Image to 3D Objects with.pdf:application/pdf}
}

@misc{tochilkin_triposr_2024,
  title      = {{TripoSR}: {Fast} {3D} {Object} {Reconstruction} from a {Single} {Image}},
  shorttitle = {{TripoSR}},
  url        = {http://arxiv.org/abs/2403.02151},
  abstract   = {This technical report introduces TripoSR, a 3D reconstruction model leveraging transformer architecture for fast feed-forward 3D generation, producing 3D mesh from a single image in under 0.5 seconds. Building upon the LRM network architecture, TripoSR integrates substantial improvements in data processing, model design, and training techniques. Evaluations on public datasets show that TripoSR exhibits superior performance, both quantitatively and qualitatively, compared to other open-source alternatives. Released under the MIT license, TripoSR is intended to empower researchers, developers, and creatives with the latest advancements in 3D generative AI.},
  urldate    = {2024-03-18},
  publisher  = {arXiv},
  author     = {Tochilkin, Dmitry and Pankratz, David and Liu, Zexiang and Huang, Zixuan and Letts, Adam and Li, Yangguang and Liang, Ding and Laforte, Christian and Jampani, Varun and Cao, Yan-Pei},
  month      = mar,
  year       = {2024},
  note       = {arXiv:2403.02151 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: Model: https://huggingface.co/stabilityai/TripoSR Code: https://github.com/VAST-AI-Research/TripoSR Demo: https://huggingface.co/spaces/stabilityai/TripoSR},
  file       = {arXiv.org Snapshot:/Users/felix/Zotero/storage/AVGNJYZY/2403.html:text/html;Full Text PDF:/Users/felix/Zotero/storage/74H5KZ7G/Tochilkin et al. - 2024 - TripoSR Fast 3D Object Reconstruction from a Sing.pdf:application/pdf}
}

@misc{hong_lrm_2024,
  title      = {{LRM}: {Large} {Reconstruction} {Model} for {Single} {Image} to {3D}},
  shorttitle = {{LRM}},
  url        = {http://arxiv.org/abs/2311.04400},
  abstract   = {We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D meshes can be found on our LRM project webpage: https://yiconghong.me/LRM.},
  urldate    = {2024-03-18},
  publisher  = {arXiv},
  author     = {Hong, Yicong and Zhang, Kai and Gu, Jiuxiang and Bi, Sai and Zhou, Yang and Liu, Difan and Liu, Feng and Sunkavalli, Kalyan and Bui, Trung and Tan, Hao},
  month      = mar,
  year       = {2024},
  note       = {arXiv:2311.04400 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
  annote     = {Comment: ICLR 2024},
  file       = {arXiv.org Snapshot:/Users/felix/Zotero/storage/ICLE8BJP/2311.html:text/html;Full Text PDF:/Users/felix/Zotero/storage/Y44HNX5J/Hong et al. - 2024 - LRM Large Reconstruction Model for Single Image t.pdf:application/pdf}
}

@inproceedings{deitke_objaverse-xl_2023,
  title     = {Objaverse-{XL}: {A} {Universe} of {10M}+ {3D} {Objects}},
  abstract  = {Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-XL. We show that by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-XL will enable further innovations in the field of 3D vision at scale.},
  language  = {en},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 37},
  author    = {Deitke, Matt and Liu, Ruoshi and Wallingford, Matthew and Ngo, Huong and Michel, Oscar and Kusupati, Aditya and Fan, Alan and Laforte, Christian and Voleti, Vikram and Gadre, Samir Yitzhak and VanderBilt, Eli and Kembhavi, Aniruddha and Vondrick, Carl and Gkioxari, Georgia and Ehsani, Kiana and Schmidt, Ludwig and Farhadi, Ali},
  year      = {2023},
  file      = {Deitke et al. - Objaverse-XL A Universe of 10M+ 3D Objects.pdf:/Users/felix/Zotero/storage/XKFV37YN/Deitke et al. - Objaverse-XL A Universe of 10M+ 3D Objects.pdf:application/pdf}
}

@article{cao_survey_2024,
  title    = {A {Survey} on {Generative} {Diffusion} {Models}},
  issn     = {1041-4347, 1558-2191, 2326-3865},
  url      = {https://ieeexplore.ieee.org/document/10419041/},
  doi      = {10.1109/TKDE.2024.3361474},
  abstract = {Deep generative models have unlocked another profound realm of human creativity. By capturing and generalizing patterns within data, we have entered the epoch of all-encompassing Artiﬁcial Intelligence for General Creativity (AIGC). Notably, diffusion models, recognized as one of the paramount generative models, materialize human ideation into tangible instances across diverse domains, encompassing imagery, text, speech, biology, and healthcare. To provide advanced and comprehensive insights into diffusion, this survey comprehensively elucidates its developmental trajectory and future directions from three distinct angles: the fundamental formulation of diffusion, algorithmic enhancements, and the manifold applications of diffusion. Each layer is meticulously explored to offer a profound comprehension of its evolution. Structured and summarized approaches are presented here.},
  language = {en},
  urldate  = {2024-03-18},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  author   = {Cao, Hanqun and Tan, Cheng and Gao, Zhangyang and Xu, Yilun and Chen, Guangyong and Heng, Pheng-Ann and Li, Stan Z.},
  year     = {2024},
  pages    = {1--20},
  file     = {Cao et al. - 2024 - A Survey on Generative Diffusion Models.pdf:/Users/felix/Zotero/storage/MF8F26CT/Cao et al. - 2024 - A Survey on Generative Diffusion Models.pdf:application/pdf}
}

@inproceedings{liu_zero-1--3_2023,
  title      = {Zero-1-to-3: {Zero}-shot {One} {Image} to {3D} {Object}},
  shorttitle = {Zero-1-to-3},
  url        = {https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Zero-1-to-3_Zero-shot_One_Image_to_3D_Object_ICCV_2023_paper.html},
  language   = {en},
  urldate    = {2024-03-31},
  author     = {Liu, Ruoshi and Wu, Rundi and Van Hoorick, Basile and Tokmakov, Pavel and Zakharov, Sergey and Vondrick, Carl},
  year       = {2023},
  pages      = {9298--9309},
  file       = {Full Text PDF:/Users/felix/Zotero/storage/8NNXIDZF/Liu et al. - 2023 - Zero-1-to-3 Zero-shot One Image to 3D Object.pdf:application/pdf}
}

@misc{poole_dreamfusion_2022,
  title      = {{DreamFusion}: {Text}-to-{3D} using {2D} {Diffusion}},
  shorttitle = {{DreamFusion}},
  url        = {http://arxiv.org/abs/2209.14988},
  doi        = {10.48550/arXiv.2209.14988},
  abstract   = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
  urldate    = {2024-03-31},
  publisher  = {arXiv},
  author     = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
  month      = sep,
  year       = {2022},
  note       = {arXiv:2209.14988 [cs, stat]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
  annote     = {Comment: see project page at https://dreamfusion3d.github.io/},
  file       = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/CMMT2IN9/Poole et al. - 2022 - DreamFusion Text-to-3D using 2D Diffusion.pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/YPRB3S6R/2209.html:text/html}
}

@misc{lin_magic3d_2023,
  title      = {{Magic3D}: {High}-{Resolution} {Text}-to-{3D} {Content} {Creation}},
  shorttitle = {{Magic3D}},
  url        = {http://arxiv.org/abs/2211.10440},
  doi        = {10.48550/arXiv.2211.10440},
  abstract   = {DreamFusion has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF), achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2x faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7\% raters to prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications.},
  urldate    = {2024-03-31},
  publisher  = {arXiv},
  author     = {Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},
  month      = mar,
  year       = {2023},
  note       = {arXiv:2211.10440 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
  annote     = {Comment: Accepted to CVPR 2023 as highlight. Project website: https://research.nvidia.com/labs/dir/magic3d},
  file       = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/533GFXY3/Lin et al. - 2023 - Magic3D High-Resolution Text-to-3D Content Creati.pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/6WIK8CJA/2211.html:text/html}
}

@inproceedings{kerr_lerf_2023,
  title      = {{LERF}: {Language} {Embedded} {Radiance} {Fields}},
  shorttitle = {{LERF}},
  url        = {https://openaccess.thecvf.com/content/ICCV2023/html/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.html?trk=public_post_comment-text},
  language   = {en},
  urldate    = {2024-03-31},
  author     = {Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
  year       = {2023},
  pages      = {19729--19739},
  file       = {Full Text PDF:/Users/felix/Zotero/storage/Q9X4DZMN/Kerr et al. - 2023 - LERF Language Embedded Radiance Fields.pdf:application/pdf}
}

@misc{wang_neus_2023,
  title      = {{NeuS}: {Learning} {Neural} {Implicit} {Surfaces} by {Volume} {Rendering} for {Multi}-view {Reconstruction}},
  shorttitle = {{NeuS}},
  url        = {http://arxiv.org/abs/2106.10689},
  doi        = {10.48550/arXiv.2106.10689},
  abstract   = {We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.},
  urldate    = {2024-03-31},
  publisher  = {arXiv},
  author     = {Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
  month      = feb,
  year       = {2023},
  note       = {arXiv:2106.10689 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
  annote     = {Comment: 23 pages},
  file       = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/ZYY7TVYQ/Wang et al. - 2023 - NeuS Learning Neural Implicit Surfaces by Volume .pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/727AC6BW/2106.html:text/html}
}

@article{muller_instant_2022,
  title    = {Instant neural graphics primitives with a multiresolution hash encoding},
  volume   = {41},
  issn     = {0730-0301},
  url      = {https://dl.acm.org/doi/10.1145/3528223.3530127},
  doi      = {10.1145/3528223.3530127},
  abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
  number   = {4},
  urldate  = {2024-03-31},
  journal  = {ACM Transactions on Graphics},
  author   = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  month    = jul,
  year     = {2022},
  keywords = {encodings, function approximation, GPUs, hashing, image synthesis, neural networks, parallel computation},
  pages    = {102:1--102:15},
  file     = {Full Text PDF:/Users/felix/Zotero/storage/5ZIA5IMS/Müller et al. - 2022 - Instant neural graphics primitives with a multires.pdf:application/pdf}
}

@inproceedings{wang_3dn_2019,
  title      = {{3DN}: {3D} {Deformation} {Network}},
  shorttitle = {{3DN}},
  url        = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_3DN_3D_Deformation_Network_CVPR_2019_paper.html},
  urldate    = {2024-04-10},
  author     = {Wang, Weiyue and Ceylan, Duygu and Mech, Radomir and Neumann, Ulrich},
  year       = {2019},
  pages      = {1038--1046},
  file       = {Full Text PDF:/Users/felix/Zotero/storage/8BKI89FD/Wang et al. - 2019 - 3DN 3D Deformation Network.pdf:application/pdf}
}

@article{fu_geo-neus_2022,
  title      = {Geo-{Neus}: {Geometry}-{Consistent} {Neural} {Implicit} {Surfaces} {Learning} for {Multi}-view {Reconstruction}},
  volume     = {35},
  shorttitle = {Geo-{Neus}},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/16415eed5a0a121bfce79924db05d3fe-Abstract-Conference.html},
  language   = {en},
  urldate    = {2024-04-10},
  journal    = {Advances in Neural Information Processing Systems},
  author     = {Fu, Qiancheng and Xu, Qingshan and Ong, Yew Soon and Tao, Wenbing},
  month      = dec,
  year       = {2022},
  pages      = {3403--3416},
  file       = {Full Text PDF:/Users/felix/Zotero/storage/YYTBXCS5/Fu et al. - 2022 - Geo-Neus Geometry-Consistent Neural Implicit Surf.pdf:application/pdf}
}

@inproceedings{pan_deep_2019,
  title   = {Deep {Mesh} {Reconstruction} {From} {Single} {RGB} {Images} via {Topology} {Modification} {Networks}},
  url     = {https://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Deep_Mesh_Reconstruction_From_Single_RGB_Images_via_Topology_Modification_ICCV_2019_paper.html},
  urldate = {2024-04-11},
  author  = {Pan, Junyi and Han, Xiaoguang and Chen, Weikai and Tang, Jiapeng and Jia, Kui},
  year    = {2019},
  pages   = {9964--9973},
  file    = {Full Text PDF:/Users/felix/Zotero/storage/5U2EDZ3D/Pan et al. - 2019 - Deep Mesh Reconstruction From Single RGB Images vi.pdf:application/pdf}
}

@book{horn_shape_1989,
  title     = {Shape from {Shading}},
  volume    = {2},
  abstract  = {The image of the world projected onto the retina is essentially two-dimensional. From this image we recover information about the shapes of objects in a three-dimensional world. How is this done?
               
               The answer lies in part in the variation of brightness, or shading, often exhibited in a region of an image. In a photograph of a face, for example, there are variations in brightness, even though the reflecting properties of the skin presumably do not vary much from place to place. It may be concluded that shading effects arise primarily because some parts of a surface are oriented so as to reflect more of the incident light toward the viewer than are others.
               
               It should be pointed out right away that the recovery of shape from shading is by no means trivial. We cannot simply associate a given image brightness with a particular surface orientation. The problem is that there are two degrees of freedom to surface orientation - it takes two numbers to specify the direction of a unit vector perpendicular to the surface. Since we have only one brightness measurement at each picture cell, we have one equation in two unknowns at every point in the image. Additional constraint must therefore be brought to bear. One way to provide the needed constraint is to assume that the surface is continuous and smooth, so that the surface orientations of neighboring surface patches are not independent.
               
               Note that there is no magic at work here: we are not recovering a function of three variables given only a function of two variables. The distribution of some absorbing material in three-dimensional space cannot be recovered from a single two-dimensional projection. The techniques of tomographic reconstruction can be applied to that problem, but only if a large number of images taken from many different viewpoints are available. Why then are we able to learn so much about the three-dimensional world from merely two-dimensional images?},
  publisher = {MIT Press},
  author    = {Horn, Berthold and Brooks, Michael},
  month     = jan,
  year      = {1989},
  file      = {Full Text PDF:/Users/felix/Zotero/storage/SWMEEDE8/Horn and Brooks - 1989 - Shape from Shading.pdf:application/pdf}
}

@inproceedings{seitz_comparison_2006,
  address   = {New York, NY, USA},
  title     = {A {Comparison} and {Evaluation} of {Multi}-{View} {Stereo} {Reconstruction} {Algorithms}},
  volume    = {1},
  isbn      = {978-0-7695-2597-6},
  url       = {http://ieeexplore.ieee.org/document/1640800/},
  doi       = {10.1109/CVPR.2006.19},
  abstract  = {This paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. Until now, the lack of suitable calibrated multi-view image datasets with known ground truth (3D shape models) has prevented such direct comparisons. In this paper, we ﬁrst survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. We then describe our process for acquiring and calibrating multiview image datasets with high-accuracy ground truth and introduce our evaluation methodology. Finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. The datasets, evaluation details, and instructions for submitting new models are available online at http://vision.middlebury.edu/mview.},
  language  = {en},
  urldate   = {2024-05-31},
  booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Volume} 1 ({CVPR}'06)},
  publisher = {IEEE},
  author    = {Seitz, S.M. and Curless, B. and Diebel, J. and Scharstein, D. and Szeliski, R.},
  year      = {2006},
  pages     = {519--528},
  file      = {Seitz et al. - 2006 - A Comparison and Evaluation of Multi-View Stereo R.pdf:/Users/felix/Zotero/storage/D8R3B2JM/Seitz et al. - 2006 - A Comparison and Evaluation of Multi-View Stereo R.pdf:application/pdf}
}

@article{furukawa_accurate_2010,
  title     = {Accurate, {Dense}, and {Robust} {Multiview} {Stereopsis}},
  volume    = {32},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  issn      = {0162-8828},
  url       = {http://ieeexplore.ieee.org/document/5226635/},
  doi       = {10.1109/TPAMI.2009.161},
  abstract  = {This paper proposes a novel algorithm for multiview stereopsis that outputs a dense set of small rectangular patches covering the surfaces visible in the images. Stereopsis is implemented as a match, expand, and filter procedure, starting from a sparse set of matched keypoints, and repeatedly expanding these before using visibility constraints to filter away false matches. The keys to the performance of the proposed algorithm are effective techniques for enforcing local photometric consistency and global visibility constraints. Simple but effective methods are also proposed to turn the resulting patch model into a mesh which can be further refined by an algorithm that enforces both photometric consistency and regularization constraints. The proposed approach automatically detects and discards outliers and obstacles and does not require any initialization in the form of a visual hull, a bounding box, or valid depth ranges. We have tested our algorithm on various data sets including objects with fine surface details, deep concavities, and thin structures, outdoor scenes observed from a restricted set of viewpoints, and “crowded” scenes where moving obstacles appear in front of a static structure of interest. A quantitative evaluation on the Middlebury benchmark [1] shows that the proposed method outperforms all others submitted so far for four out of the six data sets.},
  language  = {en},
  number    = {8},
  urldate   = {2024-05-31},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author    = {Furukawa, Yasutaka and Ponce, Jean},
  month     = aug,
  year      = {2010},
  pages     = {1362--1376},
  file      = {Furukawa and Ponce - 2010 - Accurate, Dense, and Robust Multiview Stereopsis.pdf:/Users/felix/Zotero/storage/GWHI5VUW/Furukawa and Ponce - 2010 - Accurate, Dense, and Robust Multiview Stereopsis.pdf:application/pdf}
}

@inproceedings{krizhevsky_imagenet_2012,
  title     = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
  volume    = {25},
  url       = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  abstract  = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  urldate   = {2024-05-31},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year      = {2012},
  file      = {Full Text PDF:/Users/felix/Zotero/storage/C9LGY9DE/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf}
}

@inproceedings{zhirong_wu_3d_2015,
  address    = {Boston, MA, USA},
  title      = {{3D} {ShapeNets}: {A} deep representation for volumetric shapes},
  isbn       = {978-1-4673-6964-0},
  shorttitle = {{3D} {ShapeNets}},
  url        = {https://ieeexplore.ieee.org/document/7298801/},
  doi        = {10.1109/CVPR.2015.7298801},
  language   = {en},
  urldate    = {2024-05-31},
  booktitle  = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {{Zhirong Wu} and Song, Shuran and Khosla, Aditya and {Fisher Yu} and {Linguang Zhang} and {Xiaoou Tang} and Xiao, Jianxiong},
  month      = jun,
  year       = {2015},
  pages      = {1912--1920},
  file       = {Zhirong Wu et al. - 2015 - 3D ShapeNets A deep representation for volumetric.pdf:/Users/felix/Zotero/storage/QMFIM3UQ/Zhirong Wu et al. - 2015 - 3D ShapeNets A deep representation for volumetric.pdf:application/pdf}
}

@inproceedings{charles_pointnet_2017,
  address    = {Honolulu, HI},
  title      = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
  isbn       = {978-1-5386-0457-1},
  shorttitle = {{PointNet}},
  url        = {http://ieeexplore.ieee.org/document/8099499/},
  doi        = {10.1109/CVPR.2017.16},
  abstract   = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a uniﬁed architecture for applications ranging from object classiﬁcation, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efﬁcient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  language   = {en},
  urldate    = {2024-05-31},
  booktitle  = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Charles, R. Qi and Su, Hao and Kaichun, Mo and Guibas, Leonidas J.},
  month      = jul,
  year       = {2017},
  pages      = {77--85},
  file       = {Charles et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf:/Users/felix/Zotero/storage/SU76WMYE/Charles et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf:application/pdf}
}

@inproceedings{wang_pixel2mesh_2018,
  title      = {{Pixel2Mesh}: {Generating} {3D} {Mesh} {Models} from {Single} {RGB} {Images}},
  shorttitle = {{Pixel2Mesh}},
  url        = {https://openaccess.thecvf.com/content_ECCV_2018/html/Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper.html},
  urldate    = {2024-06-01},
  author     = {Wang, Nanyang and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei and Liu, Wei and Jiang, Yu-Gang},
  year       = {2018},
  pages      = {52--67},
  file       = {Full Text PDF:/Users/felix/Zotero/storage/67WU2XJM/Wang et al. - 2018 - Pixel2Mesh Generating 3D Mesh Models from Single .pdf:application/pdf}
}

@inproceedings{park_deepsdf_2019,
  address    = {Long Beach, CA, USA},
  title      = {{DeepSDF}: {Learning} {Continuous} {Signed} {Distance} {Functions} for {Shape} {Representation}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {978-1-72813-293-8},
  shorttitle = {{DeepSDF}},
  url        = {https://ieeexplore.ieee.org/document/8954065/},
  doi        = {10.1109/CVPR.2019.00025},
  abstract   = {Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across ﬁdelity, efﬁciency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape’s surface by a continuous volumetric ﬁeld: the magnitude of a point in the ﬁeld represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape’s boundary as the zero-level-set of the learned function while explicitly representing the classiﬁcation of space as being part of the shapes’ interior or not. While classical SDF’s both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show stateof-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.},
  language   = {en},
  urldate    = {2024-06-01},
  booktitle  = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
  month      = jun,
  year       = {2019},
  pages      = {165--174},
  file       = {Park et al. - 2019 - DeepSDF Learning Continuous Signed Distance Funct.pdf:/Users/felix/Zotero/storage/JERQCY7Q/Park et al. - 2019 - DeepSDF Learning Continuous Signed Distance Funct.pdf:application/pdf}
}

@article{mildenhall_nerf_2021,
  title      = {{NeRF}: representing scenes as neural radiance fields for view synthesis},
  volume     = {65},
  issn       = {0001-0782},
  shorttitle = {{NeRF}},
  url        = {https://dl.acm.org/doi/10.1145/3503250},
  doi        = {10.1145/3503250},
  abstract   = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
  number     = {1},
  urldate    = {2024-06-01},
  journal    = {Communications of the ACM},
  author     = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  month      = dec,
  year       = {2021},
  pages      = {99--106},
  file       = {Full Text PDF:/Users/felix/Zotero/storage/5PAZVEGR/Mildenhall et al. - 2021 - NeRF representing scenes as neural radiance field.pdf:application/pdf}
}
