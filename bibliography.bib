
@inproceedings{liu_one-2-3-45_2023,
	title = {One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/4683beb6bab325650db13afd05d1a14a-Abstract-Conference.html},
	doi = {10.48550/arXiv.2306.16928},
	abstract = {Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an {SDF}-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360degree meshes. Without costly optimizations, our method reconstructs 3D shapes in significantly less time than existing methods. Moreover, our method favors better geometry, generates more 3D consistent results, and adheres more closely to the input image. We evaluate our approach on both synthetic data and in-the-wild images and demonstrate its superiority in terms of both mesh quality and runtime. In addition, our approach can seamlessly support the text-to-3D task by integrating with off-the-shelf text-to-image diffusion models.},
	booktitle = {Advances in Neural Information Processing Systems 36},
	author = {Liu, Minghua and Xu, Chao and Jin, Haian and Chen, Linghao and Varma T, Mukund and Xu, Zexiang and Su, Hao},
	date = {2023-06-29},
	file = {Liu et al. - One-2-3-45 Any Single Image to 3D Mesh in 45 Seco.pdf:/Users/felix/Zotero/storage/UUVV6DEK/Liu et al. - One-2-3-45 Any Single Image to 3D Mesh in 45 Seco.pdf:application/pdf},
}

@misc{tochilkin_triposr_2024,
	title = {{TripoSR}: Fast 3D Object Reconstruction from a Single Image},
	url = {http://arxiv.org/abs/2403.02151},
	doi = {10.48550/arXiv.2403.02151},
	shorttitle = {{TripoSR}},
	abstract = {This technical report introduces {TripoSR}, a 3D reconstruction model leveraging transformer architecture for fast feed-forward 3D generation, producing 3D mesh from a single image in under 0.5 seconds. Building upon the {LRM} network architecture, {TripoSR} integrates substantial improvements in data processing, model design, and training techniques. Evaluations on public datasets show that {TripoSR} exhibits superior performance, both quantitatively and qualitatively, compared to other open-source alternatives. Released under the {MIT} license, {TripoSR} is intended to empower researchers, developers, and creatives with the latest advancements in 3D generative {AI}.},
	author = {Tochilkin, Dmitry and Pankratz, David and Liu, Zexiang and Huang, Zixuan and Letts, Adam and Li, Yangguang and Liang, Ding and Laforte, Christian and Jampani, Varun and Cao, Yan-Pei},
	urldate = {2024-03-18},
	date = {2024-03-04},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/felix/Zotero/storage/AVGNJYZY/2403.html:text/html;Full Text PDF:/Users/felix/Zotero/storage/74H5KZ7G/Tochilkin et al. - 2024 - TripoSR Fast 3D Object Reconstruction from a Sing.pdf:application/pdf},
}

@misc{hong_lrm_2024,
	title = {{LRM}: Large Reconstruction Model for Single Image to 3D},
	url = {http://arxiv.org/abs/2311.04400},
	doi = {10.48550/arXiv.2311.04400},
	shorttitle = {{LRM}},
	abstract = {We propose the first Large Reconstruction Model ({LRM}) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as {ShapeNet} in a category-specific fashion, {LRM} adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field ({NeRF}) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from {MVImgNet}. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D meshes can be found on our {LRM} project webpage: https://yiconghong.me/{LRM}.},
	author = {Hong, Yicong and Zhang, Kai and Gu, Jiuxiang and Bi, Sai and Zhou, Yang and Liu, Difan and Liu, Feng and Sunkavalli, Kalyan and Bui, Trung and Tan, Hao},
	urldate = {2024-03-18},
	date = {2024-03-09},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/felix/Zotero/storage/ICLE8BJP/2311.html:text/html;Full Text PDF:/Users/felix/Zotero/storage/Y44HNX5J/Hong et al. - 2024 - LRM Large Reconstruction Model for Single Image t.pdf:application/pdf},
}

@inproceedings{deitke_objaverse-xl_2023,
	title = {Objaverse-{XL}: A Universe of 10M+ 3D Objects},
	doi = {10.48550/arXiv.2307.05663},
	abstract = {Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-{XL}, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-{XL} enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-{XL}. We show that by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-{XL} will enable further innovations in the field of 3D vision at scale.},
	booktitle = {Advances in Neural Information Processing Systems 37},
	author = {Deitke, Matt and Liu, Ruoshi and Wallingford, Matthew and Ngo, Huong and Michel, Oscar and Kusupati, Aditya and Fan, Alan and Laforte, Christian and Voleti, Vikram and Gadre, Samir Yitzhak and {VanderBilt}, Eli and Kembhavi, Aniruddha and Vondrick, Carl and Gkioxari, Georgia and Ehsani, Kiana and Schmidt, Ludwig and Farhadi, Ali},
	date = {2023},
	langid = {english},
	file = {Deitke et al. - Objaverse-XL A Universe of 10M+ 3D Objects.pdf:/Users/felix/Zotero/storage/XKFV37YN/Deitke et al. - Objaverse-XL A Universe of 10M+ 3D Objects.pdf:application/pdf},
}

@article{cao_survey_2024,
	title = {A Survey on Generative Diffusion Models},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/10419041/},
	doi = {10.1109/TKDE.2024.3361474},
	abstract = {Deep generative models have unlocked another profound realm of human creativity. By capturing and generalizing patterns within data, we have entered the epoch of all-encompassing ArtiÔ¨Åcial Intelligence for General Creativity ({AIGC}). Notably, diffusion models, recognized as one of the paramount generative models, materialize human ideation into tangible instances across diverse domains, encompassing imagery, text, speech, biology, and healthcare. To provide advanced and comprehensive insights into diffusion, this survey comprehensively elucidates its developmental trajectory and future directions from three distinct angles: the fundamental formulation of diffusion, algorithmic enhancements, and the manifold applications of diffusion. Each layer is meticulously explored to offer a profound comprehension of its evolution. Structured and summarized approaches are presented here.},
	pages = {1--20},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Cao, Hanqun and Tan, Cheng and Gao, Zhangyang and Xu, Yilun and Chen, Guangyong and Heng, Pheng-Ann and Li, Stan Z.},
	urldate = {2024-03-18},
	date = {2024},
	langid = {english},
	file = {Cao et al. - 2024 - A Survey on Generative Diffusion Models.pdf:/Users/felix/Zotero/storage/MF8F26CT/Cao et al. - 2024 - A Survey on Generative Diffusion Models.pdf:application/pdf},
}

@inproceedings{liu_zero-1--3_2023,
	title = {Zero-1-to-3: Zero-shot One Image to 3D Object},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Zero-1-to-3_Zero-shot_One_Image_to_3D_Object_ICCV_2023_paper.html},
	doi = {10.1109/ICCV51070.2023.00853},
	shorttitle = {Zero-1-to-3},
	eventtitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
	pages = {9298--9309},
	author = {Liu, Ruoshi and Wu, Rundi and Van Hoorick, Basile and Tokmakov, Pavel and Zakharov, Sergey and Vondrick, Carl},
	urldate = {2024-03-31},
	date = {2023},
	langid = {english},
	file = {Full Text PDF:/Users/felix/Zotero/storage/8NNXIDZF/Liu et al. - 2023 - Zero-1-to-3 Zero-shot One Image to 3D Object.pdf:application/pdf},
}

@misc{poole_dreamfusion_2022,
	title = {{DreamFusion}: Text-to-3D using 2D Diffusion},
	url = {http://arxiv.org/abs/2209.14988},
	doi = {10.48550/arXiv.2209.14988},
	shorttitle = {{DreamFusion}},
	abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a {DeepDream}-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or {NeRF}) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
	number = {{arXiv}:2209.14988},
	publisher = {{arXiv}},
	author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
	urldate = {2024-03-31},
	date = {2022-09-29},
	eprinttype = {arxiv},
	eprint = {2209.14988 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/CMMT2IN9/Poole et al. - 2022 - DreamFusion Text-to-3D using 2D Diffusion.pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/YPRB3S6R/2209.html:text/html},
}

@misc{lin_magic3d_2023,
	title = {Magic3D: High-Resolution Text-to-3D Content Creation},
	url = {http://arxiv.org/abs/2211.10440},
	doi = {10.48550/arXiv.2211.10440},
	shorttitle = {Magic3D},
	abstract = {{DreamFusion} has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize Neural Radiance Fields ({NeRF}), achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of {NeRF} and (b) low-resolution image space supervision on {NeRF}, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2x faster than {DreamFusion} (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7\% raters to prefer our approach over {DreamFusion}. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications.},
	number = {{arXiv}:2211.10440},
	publisher = {{arXiv}},
	author = {Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},
	urldate = {2024-03-31},
	date = {2023-03-25},
	eprinttype = {arxiv},
	eprint = {2211.10440 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/533GFXY3/Lin et al. - 2023 - Magic3D High-Resolution Text-to-3D Content Creati.pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/6WIK8CJA/2211.html:text/html},
}

@inproceedings{kerr_lerf_2023,
	title = {{LERF}: Language Embedded Radiance Fields},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.html?trk=public_post_comment-text},
	shorttitle = {{LERF}},
	eventtitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
	pages = {19729--19739},
	author = {Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
	urldate = {2024-03-31},
	date = {2023},
	langid = {english},
	file = {Full Text PDF:/Users/felix/Zotero/storage/Q9X4DZMN/Kerr et al. - 2023 - LERF Language Embedded Radiance Fields.pdf:application/pdf},
}

@misc{wang_neus_2023,
	title = {{NeuS}: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction},
	url = {http://arxiv.org/abs/2106.10689},
	doi = {10.48550/arXiv.2106.10689},
	shorttitle = {{NeuS}},
	abstract = {We present a novel neural surface reconstruction method, called {NeuS}, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as {DVR} and {IDR}, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as {NeRF} and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In {NeuS}, we propose to represent a surface as the zero-level set of a signed distance function ({SDF}) and develop a new volume rendering method to train a neural {SDF} representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the {DTU} dataset and the {BlendedMVS} dataset show that {NeuS} outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.},
	number = {{arXiv}:2106.10689},
	publisher = {{arXiv}},
	author = {Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
	urldate = {2024-03-31},
	date = {2023-02-01},
	eprinttype = {arxiv},
	eprint = {2106.10689 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/ZYY7TVYQ/Wang et al. - 2023 - NeuS Learning Neural Implicit Surfaces by Volume .pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/727AC6BW/2106.html:text/html},
}

@article{muller_instant_2022,
	title = {Instant neural graphics primitives with a multiresolution hash encoding},
	volume = {41},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/3528223.3530127},
	doi = {10.1145/3528223.3530127},
	abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern {GPUs}. We leverage this parallelism by implementing the whole system using fully-fused {CUDA} kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920√ó1080.},
	pages = {102:1--102:15},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {M√ºller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
	urldate = {2024-03-31},
	date = {2022-07-22},
	keywords = {encodings, function approximation, {GPUs}, hashing, image synthesis, neural networks, parallel computation},
	file = {Full Text PDF:/Users/felix/Zotero/storage/5ZIA5IMS/M√ºller et al. - 2022 - Instant neural graphics primitives with a multires.pdf:application/pdf},
}

@inproceedings{wang_3dn_2019,
	title = {3DN: 3D Deformation Network},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_3DN_3D_Deformation_Network_CVPR_2019_paper.html},
	doi = {10.48550/arXiv.1903.03322},
	shorttitle = {3DN},
	eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	pages = {1038--1046},
	author = {Wang, Weiyue and Ceylan, Duygu and Mech, Radomir and Neumann, Ulrich},
	urldate = {2024-04-10},
	date = {2019},
	file = {Full Text PDF:/Users/felix/Zotero/storage/8BKI89FD/Wang et al. - 2019 - 3DN 3D Deformation Network.pdf:application/pdf},
}

@article{fu_geo-neus_2022,
	title = {Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/16415eed5a0a121bfce79924db05d3fe-Abstract-Conference.html},
	shorttitle = {Geo-Neus},
	pages = {3403--3416},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Fu, Qiancheng and Xu, Qingshan and Ong, Yew Soon and Tao, Wenbing},
	urldate = {2024-04-10},
	date = {2022-12-06},
	langid = {english},
	file = {Full Text PDF:/Users/felix/Zotero/storage/YYTBXCS5/Fu et al. - 2022 - Geo-Neus Geometry-Consistent Neural Implicit Surf.pdf:application/pdf},
}

@inproceedings{pan_deep_2019,
	title = {Deep Mesh Reconstruction From Single {RGB} Images via Topology Modification Networks},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Deep_Mesh_Reconstruction_From_Single_RGB_Images_via_Topology_Modification_ICCV_2019_paper.html},
	eventtitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
	pages = {9964--9973},
	author = {Pan, Junyi and Han, Xiaoguang and Chen, Weikai and Tang, Jiapeng and Jia, Kui},
	urldate = {2024-04-11},
	date = {2019},
	file = {Full Text PDF:/Users/felix/Zotero/storage/5U2EDZ3D/Pan et al. - 2019 - Deep Mesh Reconstruction From Single RGB Images vi.pdf:application/pdf},
}

@book{horn_shape_1989,
	title = {Shape from Shading},
	volume = {2},
	url = {https://www.researchgate.net/publication/230687001_Shape_from_Shading},
	abstract = {The image of the world projected onto the retina is essentially two-dimensional. From this image we recover information about the shapes of objects in a three-dimensional world. How is this done? 

The answer lies in part in the variation of brightness, or shading, often exhibited in a region of an image. In a photograph of a face, for example, there are variations in brightness, even though the reflecting properties of the skin presumably do not vary much from place to place. It may be concluded that shading effects arise primarily because some parts of a surface are oriented so as to reflect more of the incident light toward the viewer than are others.

It should be pointed out right away that the recovery of shape from shading is by no means trivial. We cannot simply associate a given image brightness with a particular surface orientation. The problem is that there are two degrees of freedom to surface orientation - it takes two numbers to specify the direction of a unit vector perpendicular to the surface. Since we have only one brightness measurement at each picture cell, we have one equation in two unknowns at every point in the image. Additional constraint must therefore be brought to bear. One way to provide the needed constraint is to assume that the surface is continuous and smooth, so that the surface orientations of neighboring surface patches are not independent.

Note that there is no magic at work here: we are not recovering a function of three variables given only a function of two variables. The distribution of some absorbing material in three-dimensional space cannot be recovered from a single two-dimensional projection. The techniques of tomographic reconstruction can be applied to that problem, but only if a large number of images taken from many different viewpoints are available. Why then are we able to learn so much about the three-dimensional world from merely two-dimensional images?},
	publisher = {{MIT} Press},
	author = {Horn, Berthold and Brooks, Michael},
	date = {1989-01-01},
	file = {Full Text PDF:/Users/felix/Zotero/storage/SWMEEDE8/Horn and Brooks - 1989 - Shape from Shading.pdf:application/pdf},
}

@inproceedings{seitz_comparison_2006,
	location = {New York, {NY}, {USA}},
	title = {A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms},
	volume = {1},
	isbn = {978-0-7695-2597-6},
	url = {http://ieeexplore.ieee.org/document/1640800/},
	doi = {10.1109/CVPR.2006.19},
	abstract = {This paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. Until now, the lack of suitable calibrated multi-view image datasets with known ground truth (3D shape models) has prevented such direct comparisons. In this paper, we Ô¨Årst survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. We then describe our process for acquiring and calibrating multiview image datasets with high-accuracy ground truth and introduce our evaluation methodology. Finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. The datasets, evaluation details, and instructions for submitting new models are available online at http://vision.middlebury.edu/mview.},
	eventtitle = {2006 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1 ({CVPR}'06)},
	pages = {519--528},
	booktitle = {2006 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1 ({CVPR}'06)},
	publisher = {{IEEE}},
	author = {Seitz, S.M. and Curless, B. and Diebel, J. and Scharstein, D. and Szeliski, R.},
	urldate = {2024-05-31},
	date = {2006},
	langid = {english},
	file = {Seitz et al. - 2006 - A Comparison and Evaluation of Multi-View Stereo R.pdf:/Users/felix/Zotero/storage/D8R3B2JM/Seitz et al. - 2006 - A Comparison and Evaluation of Multi-View Stereo R.pdf:application/pdf},
}

@article{furukawa_accurate_2010,
	title = {Accurate, Dense, and Robust Multiview Stereopsis},
	volume = {32},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/5226635/},
	doi = {10.1109/TPAMI.2009.161},
	abstract = {This paper proposes a novel algorithm for multiview stereopsis that outputs a dense set of small rectangular patches covering the surfaces visible in the images. Stereopsis is implemented as a match, expand, and filter procedure, starting from a sparse set of matched keypoints, and repeatedly expanding these before using visibility constraints to filter away false matches. The keys to the performance of the proposed algorithm are effective techniques for enforcing local photometric consistency and global visibility constraints. Simple but effective methods are also proposed to turn the resulting patch model into a mesh which can be further refined by an algorithm that enforces both photometric consistency and regularization constraints. The proposed approach automatically detects and discards outliers and obstacles and does not require any initialization in the form of a visual hull, a bounding box, or valid depth ranges. We have tested our algorithm on various data sets including objects with fine surface details, deep concavities, and thin structures, outdoor scenes observed from a restricted set of viewpoints, and ‚Äúcrowded‚Äù scenes where moving obstacles appear in front of a static structure of interest. A quantitative evaluation on the Middlebury benchmark [1] shows that the proposed method outperforms all others submitted so far for four out of the six data sets.},
	pages = {1362--1376},
	number = {8},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Furukawa, Yasutaka and Ponce, Jean},
	urldate = {2024-05-31},
	date = {2010-08},
	langid = {english},
	file = {Furukawa and Ponce - 2010 - Accurate, Dense, and Robust Multiview Stereopsis.pdf:/Users/felix/Zotero/storage/GWHI5VUW/Furukawa and Ponce - 2010 - Accurate, Dense, and Robust Multiview Stereopsis.pdf:application/pdf},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {25},
	url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the {LSVRC}-2010 {ImageNet} training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	urldate = {2024-05-31},
	date = {2012},
	file = {Full Text PDF:/Users/felix/Zotero/storage/C9LGY9DE/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@inproceedings{zhirong_wu_3d_2015,
	location = {Boston, {MA}, {USA}},
	title = {3D {ShapeNets}: A deep representation for volumetric shapes},
	isbn = {978-1-4673-6964-0},
	url = {https://ieeexplore.ieee.org/document/7298801/},
	doi = {10.1109/CVPR.2015.7298801},
	shorttitle = {3D {ShapeNets}},
	eventtitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {1912--1920},
	booktitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {{Zhirong Wu} and Song, Shuran and Khosla, Aditya and {Fisher Yu} and {Linguang Zhang} and {Xiaoou Tang} and Xiao, Jianxiong},
	urldate = {2024-05-31},
	date = {2015-06},
	langid = {english},
	file = {Zhirong Wu et al. - 2015 - 3D ShapeNets A deep representation for volumetric.pdf:/Users/felix/Zotero/storage/QMFIM3UQ/Zhirong Wu et al. - 2015 - 3D ShapeNets A deep representation for volumetric.pdf:application/pdf},
}

@inproceedings{charles_pointnet_2017,
	location = {Honolulu, {HI}},
	title = {{PointNet}: Deep Learning on Point Sets for 3D Classification and Segmentation},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099499/},
	doi = {10.1109/CVPR.2017.16},
	shorttitle = {{PointNet}},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named {PointNet}, provides a uniÔ¨Åed architecture for applications ranging from object classiÔ¨Åcation, part segmentation, to scene semantic parsing. Though simple, {PointNet} is highly efÔ¨Åcient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {77--85},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Charles, R. Qi and Su, Hao and Kaichun, Mo and Guibas, Leonidas J.},
	urldate = {2024-05-31},
	date = {2017-07},
	langid = {english},
	file = {Charles et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf:/Users/felix/Zotero/storage/SU76WMYE/Charles et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf:application/pdf},
}

@inproceedings{wang_pixel2mesh_2018,
	title = {Pixel2Mesh: Generating 3D Mesh Models from Single {RGB} Images},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper.html},
	doi = {10.1007/978-3-030-01252-6_4},
	shorttitle = {Pixel2Mesh},
	eventtitle = {Proceedings of the European Conference on Computer Vision ({ECCV})},
	pages = {52--67},
	author = {Wang, Nanyang and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei and Liu, Wei and Jiang, Yu-Gang},
	urldate = {2024-06-01},
	date = {2018},
	file = {Full Text PDF:/Users/felix/Zotero/storage/67WU2XJM/Wang et al. - 2018 - Pixel2Mesh Generating 3D Mesh Models from Single .pdf:application/pdf},
}

@inproceedings{park_deepsdf_2019,
	location = {Long Beach, {CA}, {USA}},
	title = {{DeepSDF}: Learning Continuous Signed Distance Functions for Shape Representation},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954065/},
	doi = {10.1109/CVPR.2019.00025},
	shorttitle = {{DeepSDF}},
	abstract = {Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across Ô¨Ådelity, efÔ¨Åciency and compression capabilities. In this work, we introduce {DeepSDF}, a learned continuous Signed Distance Function ({SDF}) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. {DeepSDF}, like its classical counterpart, represents a shape‚Äôs surface by a continuous volumetric Ô¨Åeld: the magnitude of a point in the Ô¨Åeld represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape‚Äôs boundary as the zero-level-set of the learned function while explicitly representing the classiÔ¨Åcation of space as being part of the shapes‚Äô interior or not. While classical {SDF}‚Äôs both in analytical or discretized voxel form typically represent the surface of a single shape, {DeepSDF} can represent an entire class of shapes. Furthermore, we show stateof-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.},
	eventtitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {165--174},
	booktitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
	urldate = {2024-06-01},
	date = {2019-06},
	langid = {english},
	file = {Park et al. - 2019 - DeepSDF Learning Continuous Signed Distance Funct.pdf:/Users/felix/Zotero/storage/JERQCY7Q/Park et al. - 2019 - DeepSDF Learning Continuous Signed Distance Funct.pdf:application/pdf},
}

@article{mildenhall_nerf_2021,
	title = {{NeRF}: representing scenes as neural radiance fields for view synthesis},
	volume = {65},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3503250},
	doi = {10.1145/3503250},
	shorttitle = {{NeRF}},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (Œ∏, œï)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
	pages = {99--106},
	number = {1},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	urldate = {2024-06-01},
	date = {2021-12-17},
	file = {Full Text PDF:/Users/felix/Zotero/storage/5PAZVEGR/Mildenhall et al. - 2021 - NeRF representing scenes as neural radiance field.pdf:application/pdf},
}

@article{ullman_interpretation_1997,
	title = {The interpretation of structure from motion},
	volume = {203},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspb.1979.0006},
	doi = {10.1098/rspb.1979.0006},
	abstract = {The interpretation of structure from motion is examined from a computional point of view. The question addressed is how the three dimen¬≠sional structure and motion of objects can be inferred from the two dimensional transformations of their projected images when no three dimensional information is conveyed by the individual projections. The following scheme is proposed: (i) divide the image into groups of four elements each; (ii) test each group for a rigid interpretation; (iii) combine the results obtained in (ii). It is shown that this scheme will correctly decompose scenes containing arbitrary rigid objects in motion, recovering their three dimensional structure and motion. The analysis is based primarily on the  ªstructure from motion‚Äô theorem which states that the structure of four non-coplanar points is recoverable from three orthographic projections. The interpretation scheme is extended to cover perspective projections, and its psychological relevance is discussed.},
	pages = {405--426},
	number = {1153},
	journaltitle = {Proceedings of the Royal Society of London. Series B. Biological Sciences},
	author = {Ullman, S. and Brenner, Sydney},
	urldate = {2024-06-02},
	date = {1997-01},
	note = {Publisher: Royal Society},
	file = {Submitted Version:/Users/felix/Zotero/storage/HGI98PSP/Ullman and Brenner - 1997 - The interpretation of structure from motion.pdf:application/pdf},
}

@inproceedings{goodfellow_generative_2014,
	title = {Generative Adversarial Nets},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2024-06-02},
	date = {2014},
	file = {Full Text PDF:/Users/felix/Zotero/storage/9E7NYE7J/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf:application/pdf},
}

@inproceedings{wen_pixel2mesh_2019,
	location = {Seoul, Korea (South)},
	title = {Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010697/},
	doi = {10.1109/ICCV.2019.00113},
	shorttitle = {Pixel2Mesh++},
	abstract = {We study the problem of shape generation in 3D mesh representation from a few color images with known camera poses. While many previous works learn to hallucinate the shape directly from priors, we resort to further improving the shape quality by leveraging cross-view information with a graph convolutional network. Instead of building a direct mapping function from images to 3D shape, our model learns to predict series of deformations to improve a coarse shape iteratively. Inspired by traditional multiple view geometry methods, our network samples nearby area around the initial mesh‚Äôs vertex locations and reasons an optimal deformation using perceptual feature statistics built from multiple input images. Extensive experiments show that our model produces accurate 3D shape that are not only visually plausible from the input perspectives, but also well aligned to arbitrary viewpoints. With the help of physically driven architecture, our model also exhibits generalization capability across different semantic categories, number of input images, and quality of mesh initialization.},
	eventtitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {1042--1051},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Wen, Chao and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei},
	urldate = {2024-06-02},
	date = {2019-10},
	langid = {english},
	file = {Wen et al. - 2019 - Pixel2Mesh++ Multi-View 3D Mesh Generation via De.pdf:/Users/felix/Zotero/storage/9GK4MKSH/Wen et al. - 2019 - Pixel2Mesh++ Multi-View 3D Mesh Generation via De.pdf:application/pdf},
}

@article{azhar_building_2020,
	title = {Building Information Modeling ({BIM}): Now and beyond},
	volume = {12},
	url = {https://search.informit.org/doi/abs/10.3316/informit.013120167780649},
	doi = {10.5130/ajceb.v12i4.3032},
	shorttitle = {Building Information Modeling ({BIM})},
	abstract = {Building Information Modeling ({BIM}), also called n-D Modeling or Virtual Prototyping
         Technology, is a revolutionary development that is quickly reshaping the Architecture-Engineering-Construction
         ({AEC}) industry. {BIM} is both a technology and a process. The technology component of
         {BIM} helps project stakeholders to visualize what is to be built in a simulated environment
         to identify any potential design, construction or operational issues. The process
         component enables close collaboration and encourages integration of the roles of all
         stakeholders on a project. The paper presents an overview of {BIM} with focus on its
         core concepts, applications in the project life cycle and benefits for project stakeholders
         with the help of case studies. The paper also elaborates risks and barriers to {BIM}
         implementation and future trends.},
	pages = {15--28},
	number = {4},
	journaltitle = {The Australasian Journal of Construction Economics and Building},
	author = {Azhar, Salman and Khalfan, Malik and Maqsood, Tayyab},
	urldate = {2024-06-20},
	date = {2020-08-20},
	note = {Publisher: {UTS} {ePress}},
	keywords = {Building information modeling, Construction industry--Information technology, Construction industry--Management, {PASDA} (Information retrieval system), Structural engineering},
	file = {Full Text PDF:/Users/felix/Zotero/storage/QQ9V6BEL/Azhar et al. - 2020 - Building Information Modeling (BIM) Now and beyon.pdf:application/pdf},
}

@inproceedings{wolff_generalizing_1996,
	location = {Berlin, Heidelberg},
	title = {Generalizing Lambert's Law for smooth surfaces},
	isbn = {978-3-540-49950-3},
	doi = {10.1007/3-540-61123-1_126},
	abstract = {One of the most common assumptions for recovering object features in computer vision and rendering objects in computer graphics is that diffuse reflection from materials is Lambertian. This paper shows that there is significant deviation from Lambertian behavior in diffuse reflection from smooth surfaces not predicted by existing reflectance models, having an important bearing on any computer vision technique that may utilize reflectance models including shape-from-shading and binocular stereo. Contrary to prediction by Lambert's Law, diffuse reflection from smooth surfaces is significantly viewpoint dependent, and there are prominent diffuse reflection maxima effects occurring on objects when incident point source illumination is greater than 50‚Ä° relative to viewing including the range from 90‚Ä° to 180‚Ä° where the light source is behind the object with respect to viewing. Presented here is a diffuse reflectance model, derived from first physical principles, utilizing results of radiative transfer theory for subsurface multiple scattering together with Fresnel attenuation and Snell refraction at a smooth air-dielectric surface boundary. A number of experimental results are presented demonstrating striking deviation from Lambertian behavior predicted by the proposed diffuse reflectance model.},
	pages = {40--53},
	booktitle = {Computer Vision ‚Äî {ECCV} '96},
	publisher = {Springer},
	author = {Wolff, Lawrence B.},
	editor = {Buxton, Bernard and Cipolla, Roberto},
	date = {1996},
	langid = {english},
	keywords = {Dielectric Surface, Diffuse Reflection, Photometric Stereo, Reflectance Model, Single Scattering Albedo},
	file = {Full Text PDF:/Users/felix/Zotero/storage/4H7BEI32/Wolff - 1996 - Generalizing Lambert's Law for smooth surfaces.pdf:application/pdf},
}

@article{he_advances_2018,
	title = {Advances in sensing and processing methods for three-dimensional robot vision},
	volume = {15},
	doi = {10.1177/1729881418760623},
	abstract = {In this article, we survey the recent developments of sensing methods in three-dimensional robot vision, centering on the current three-dimensional sensors and core techniques embedded in robotic systems. Over 8000 publications have reported rather wide application areas of three-dimensional robot vision in the last 40 years, such as human‚Äìrobot interaction, object recognition, three-dimensional modeling, object tracking, searching and surveillance, as well as robot manipulation, localization, navigation, mapping, and path planning. Representative works and future research trends are also addressed in this article.},
	pages = {172988141876062},
	journaltitle = {International Journal of Advanced Robotic Systems},
	shortjournal = {International Journal of Advanced Robotic Systems},
	author = {He, Yu and Chen, Shengyong},
	date = {2018-03-20},
	file = {Full Text PDF:/Users/felix/Zotero/storage/25J75UIJ/He and Chen - 2018 - Advances in sensing and processing methods for thr.pdf:application/pdf},
}

@misc{chang_shapenet_2015,
	title = {{ShapeNet}: An Information-Rich 3D Model Repository},
	url = {http://arxiv.org/abs/1512.03012},
	doi = {10.48550/arXiv.1512.03012},
	shorttitle = {{ShapeNet}},
	abstract = {We present {ShapeNet}: a richly-annotated, large-scale repository of shapes represented by 3D {CAD} models of objects. {ShapeNet} contains 3D models from a multitude of semantic categories and organizes them under the {WordNet} taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, {ShapeNet} has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories ({WordNet} synsets). In this report we describe the {ShapeNet} effort as a whole, provide details for all currently available datasets, and summarize future plans.},
	author = {Chang, Angel X. and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},
	urldate = {2024-06-20},
	date = {2015-12-09},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/PE2DXENI/Chang et al. - 2015 - ShapeNet An Information-Rich 3D Model Repository.pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/M5KCIBT7/1512.html:text/html},
}

@misc{wang_instantiation-net_2019,
	title = {Instantiation-Net: 3D Mesh Reconstruction from Single 2D Image for Right Ventricle},
	url = {http://arxiv.org/abs/1909.08986},
	doi = {10.1007/978-3-030-59719-1_66},
	shorttitle = {Instantiation-Net},
	abstract = {3D shape instantiation which reconstructs the 3D shape of a target from limited 2D images or projections is an emerging technique for surgical intervention. It improves the currently less-informative and insufficient 2D navigation schemes for robot-assisted Minimally Invasive Surgery ({MIS}) to 3D navigation. Previously, a general and registration-free framework was proposed for 3D shape instantiation based on Kernel Partial Least Square Regression ({KPLSR}), requiring manually segmented anatomical structures as the pre-requisite. Two hyper-parameters including the Gaussian width and component number also need to be carefully adjusted. Deep Convolutional Neural Network ({DCNN}) based framework has also been proposed to reconstruct a 3D point cloud from a single 2D image, with end-to-end and fully automatic learning. In this paper, an Instantiation-Net is proposed to reconstruct the 3D mesh of a target from its a single 2D image, by using {DCNN} to extract features from the 2D image and Graph Convolutional Network ({GCN}) to reconstruct the 3D mesh, and using Fully Connected ({FC}) layers to connect the {DCNN} to {GCN}. Detailed validation was performed to demonstrate the practical strength of the method and its potential clinical use.},
	author = {Wang, Zhao-Yang and Zhou, Xiao-Yun and Li, Peichao and Riga, Celia and Yang, Guang-Zhong},
	urldate = {2024-06-20},
	date = {2019-09-16},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/C3VMSB67/Wang et al. - 2019 - Instantiation-Net 3D Mesh Reconstruction from Sin.pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/IMBFIMJ8/1909.html:text/html},
}

@online{rouch_patchmatch_2023,
	title = {{PatchMatch} Multi-View Stereo},
	url = {https://betterprogramming.pub/patchmatch-multi-view-stereo-1-2-fc46e5dfe912},
	abstract = {Understand how the {MVS} tool provided by {COLMAP} works.},
	titleaddon = {Medium},
	author = {Rouch, Thomas},
	urldate = {2024-06-30},
	date = {2023-04-10},
	langid = {english},
	file = {Snapshot:/Users/felix/Zotero/storage/UDH56IB8/patchmatch-multi-view-stereo-1-2-fc46e5dfe912.html:text/html},
}

@inproceedings{liu_one-2-3-45_2024,
	title = {One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Liu_One-2-3-45_Fast_Single_Image_to_3D_Objects_with_Consistent_Multi-View_CVPR_2024_paper.html},
	doi = {10.48550/arXiv.2311.07885},
	shorttitle = {One-2-3-45++},
	eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	pages = {10072--10083},
	author = {Liu, Minghua and Shi, Ruoxi and Chen, Linghao and Zhang, Zhuoyang and Xu, Chao and Wei, Xinyue and Chen, Hansheng and Zeng, Chong and Gu, Jiayuan and Su, Hao},
	urldate = {2024-06-30},
	date = {2024},
	file = {Full Text PDF:/Users/felix/Zotero/storage/G3MKWN26/Liu et al. - 2024 - One-2-3-45++ Fast Single Image to 3D Objects with.pdf:application/pdf},
}
