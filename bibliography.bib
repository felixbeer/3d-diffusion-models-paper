
@inproceedings{liu_one-2-3-45_2023,
  title     = {One-2-3-45: {Any} {Single} {Image} to {3D} {Mesh} in 45 {Seconds} without {Per}-{Shape} {Optimization}},
  abstract  = {Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360degree meshes. Without costly optimizations, our method reconstructs 3D shapes in significantly less time than existing methods. Moreover, our method favors better geometry, generates more 3D consistent results, and adheres more closely to the input image. We evaluate our approach on both synthetic data and in-the-wild images and demonstrate its superiority in terms of both mesh quality and runtime. In addition, our approach can seamlessly support the text-to-3D task by integrating with off-the-shelf text-to-image diffusion models.},
  language  = {en},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 36},
  author    = {Liu, Minghua and Xu, Chao and Jin, Haian and Chen, Linghao and Varma T, Mukund and Xu, Zexiang and Su, Hao},
  month     = jun,
  year      = {2023},
  file      = {Liu et al. - One-2-3-45 Any Single Image to 3D Mesh in 45 Seco.pdf:/Users/felix/Zotero/storage/UUVV6DEK/Liu et al. - One-2-3-45 Any Single Image to 3D Mesh in 45 Seco.pdf:application/pdf}
}

@misc{liu_one-2-3-45_2023-1,
  title      = {One-2-3-45++: {Fast} {Single} {Image} to {3D} {Objects} with {Consistent} {Multi}-{View} {Generation} and {3D} {Diffusion}},
  shorttitle = {One-2-3-45++},
  url        = {http://arxiv.org/abs/2311.07885},
  abstract   = {Recent advancements in open-world 3D object generation have been remarkable, with image-to-3D methods offering superior fine-grained control over their text-to-3D counterparts. However, most existing models fall short in simultaneously providing rapid generation speeds and high fidelity to input images - two features essential for practical applications. In this paper, we present One-2-3-45++, an innovative method that transforms a single image into a detailed 3D textured mesh in approximately one minute. Our approach aims to fully harness the extensive knowledge embedded in 2D diffusion models and priors from valuable yet limited 3D data. This is achieved by initially finetuning a 2D diffusion model for consistent multi-view image generation, followed by elevating these images to 3D with the aid of multi-view conditioned 3D native diffusion models. Extensive experimental evaluations demonstrate that our method can produce high-quality, diverse 3D assets that closely mirror the original input image. Our project webpage: https://sudo-ai-3d.github.io/One2345plus\_page.},
  urldate    = {2024-03-18},
  publisher  = {arXiv},
  author     = {Liu, Minghua and Shi, Ruoxi and Chen, Linghao and Zhang, Zhuoyang and Xu, Chao and Wei, Xinyue and Chen, Hansheng and Zeng, Chong and Gu, Jiayuan and Su, Hao},
  month      = nov,
  year       = {2023},
  note       = {arXiv:2311.07885 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
  file       = {arXiv.org Snapshot:/Users/felix/Zotero/storage/P33PW48P/2311.html:text/html;Full Text PDF:/Users/felix/Zotero/storage/ZVULD83E/Liu et al. - 2023 - One-2-3-45++ Fast Single Image to 3D Objects with.pdf:application/pdf}
}

@misc{tochilkin_triposr_2024,
  title      = {{TripoSR}: {Fast} {3D} {Object} {Reconstruction} from a {Single} {Image}},
  shorttitle = {{TripoSR}},
  url        = {http://arxiv.org/abs/2403.02151},
  abstract   = {This technical report introduces TripoSR, a 3D reconstruction model leveraging transformer architecture for fast feed-forward 3D generation, producing 3D mesh from a single image in under 0.5 seconds. Building upon the LRM network architecture, TripoSR integrates substantial improvements in data processing, model design, and training techniques. Evaluations on public datasets show that TripoSR exhibits superior performance, both quantitatively and qualitatively, compared to other open-source alternatives. Released under the MIT license, TripoSR is intended to empower researchers, developers, and creatives with the latest advancements in 3D generative AI.},
  urldate    = {2024-03-18},
  publisher  = {arXiv},
  author     = {Tochilkin, Dmitry and Pankratz, David and Liu, Zexiang and Huang, Zixuan and Letts, Adam and Li, Yangguang and Liang, Ding and Laforte, Christian and Jampani, Varun and Cao, Yan-Pei},
  month      = mar,
  year       = {2024},
  note       = {arXiv:2403.02151 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: Model: https://huggingface.co/stabilityai/TripoSR Code: https://github.com/VAST-AI-Research/TripoSR Demo: https://huggingface.co/spaces/stabilityai/TripoSR},
  file       = {arXiv.org Snapshot:/Users/felix/Zotero/storage/AVGNJYZY/2403.html:text/html;Full Text PDF:/Users/felix/Zotero/storage/74H5KZ7G/Tochilkin et al. - 2024 - TripoSR Fast 3D Object Reconstruction from a Sing.pdf:application/pdf}
}

@misc{hong_lrm_2024,
  title      = {{LRM}: {Large} {Reconstruction} {Model} for {Single} {Image} to {3D}},
  shorttitle = {{LRM}},
  url        = {http://arxiv.org/abs/2311.04400},
  abstract   = {We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D meshes can be found on our LRM project webpage: https://yiconghong.me/LRM.},
  urldate    = {2024-03-18},
  publisher  = {arXiv},
  author     = {Hong, Yicong and Zhang, Kai and Gu, Jiuxiang and Bi, Sai and Zhou, Yang and Liu, Difan and Liu, Feng and Sunkavalli, Kalyan and Bui, Trung and Tan, Hao},
  month      = mar,
  year       = {2024},
  note       = {arXiv:2311.04400 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
  annote     = {Comment: ICLR 2024},
  file       = {arXiv.org Snapshot:/Users/felix/Zotero/storage/ICLE8BJP/2311.html:text/html;Full Text PDF:/Users/felix/Zotero/storage/Y44HNX5J/Hong et al. - 2024 - LRM Large Reconstruction Model for Single Image t.pdf:application/pdf}
}

@inproceedings{deitke_objaverse-xl_2023,
  title     = {Objaverse-{XL}: {A} {Universe} of {10M}+ {3D} {Objects}},
  abstract  = {Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-XL. We show that by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-XL will enable further innovations in the field of 3D vision at scale.},
  language  = {en},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 37},
  author    = {Deitke, Matt and Liu, Ruoshi and Wallingford, Matthew and Ngo, Huong and Michel, Oscar and Kusupati, Aditya and Fan, Alan and Laforte, Christian and Voleti, Vikram and Gadre, Samir Yitzhak and VanderBilt, Eli and Kembhavi, Aniruddha and Vondrick, Carl and Gkioxari, Georgia and Ehsani, Kiana and Schmidt, Ludwig and Farhadi, Ali},
  year      = {2023},
  file      = {Deitke et al. - Objaverse-XL A Universe of 10M+ 3D Objects.pdf:/Users/felix/Zotero/storage/XKFV37YN/Deitke et al. - Objaverse-XL A Universe of 10M+ 3D Objects.pdf:application/pdf}
}

@article{cao_survey_2024,
  title    = {A {Survey} on {Generative} {Diffusion} {Models}},
  issn     = {1041-4347, 1558-2191, 2326-3865},
  url      = {https://ieeexplore.ieee.org/document/10419041/},
  doi      = {10.1109/TKDE.2024.3361474},
  abstract = {Deep generative models have unlocked another profound realm of human creativity. By capturing and generalizing patterns within data, we have entered the epoch of all-encompassing Artiﬁcial Intelligence for General Creativity (AIGC). Notably, diffusion models, recognized as one of the paramount generative models, materialize human ideation into tangible instances across diverse domains, encompassing imagery, text, speech, biology, and healthcare. To provide advanced and comprehensive insights into diffusion, this survey comprehensively elucidates its developmental trajectory and future directions from three distinct angles: the fundamental formulation of diffusion, algorithmic enhancements, and the manifold applications of diffusion. Each layer is meticulously explored to offer a profound comprehension of its evolution. Structured and summarized approaches are presented here.},
  language = {en},
  urldate  = {2024-03-18},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  author   = {Cao, Hanqun and Tan, Cheng and Gao, Zhangyang and Xu, Yilun and Chen, Guangyong and Heng, Pheng-Ann and Li, Stan Z.},
  year     = {2024},
  pages    = {1--20},
  file     = {Cao et al. - 2024 - A Survey on Generative Diffusion Models.pdf:/Users/felix/Zotero/storage/MF8F26CT/Cao et al. - 2024 - A Survey on Generative Diffusion Models.pdf:application/pdf}
}

@inproceedings{liu_zero-1--3_2023,
  title      = {Zero-1-to-3: {Zero}-shot {One} {Image} to {3D} {Object}},
  shorttitle = {Zero-1-to-3},
  url        = {https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Zero-1-to-3_Zero-shot_One_Image_to_3D_Object_ICCV_2023_paper.html},
  language   = {en},
  urldate    = {2024-03-31},
  author     = {Liu, Ruoshi and Wu, Rundi and Van Hoorick, Basile and Tokmakov, Pavel and Zakharov, Sergey and Vondrick, Carl},
  year       = {2023},
  pages      = {9298--9309},
  file       = {Full Text PDF:/Users/felix/Zotero/storage/8NNXIDZF/Liu et al. - 2023 - Zero-1-to-3 Zero-shot One Image to 3D Object.pdf:application/pdf}
}

@misc{mildenhall_nerf_2020,
  title      = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
  shorttitle = {{NeRF}},
  url        = {http://arxiv.org/abs/2003.08934},
  doi        = {10.48550/arXiv.2003.08934},
  abstract   = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  urldate    = {2024-03-31},
  publisher  = {arXiv},
  author     = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  month      = aug,
  year       = {2020},
  note       = {arXiv:2003.08934 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
  annote     = {Comment: ECCV 2020 (oral). Project page with videos and code: http://tancik.com/nerf},
  file       = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/889T5R9Y/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/ETR99UY3/2003.html:text/html}
}

@misc{poole_dreamfusion_2022,
  title      = {{DreamFusion}: {Text}-to-{3D} using {2D} {Diffusion}},
  shorttitle = {{DreamFusion}},
  url        = {http://arxiv.org/abs/2209.14988},
  doi        = {10.48550/arXiv.2209.14988},
  abstract   = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
  urldate    = {2024-03-31},
  publisher  = {arXiv},
  author     = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
  month      = sep,
  year       = {2022},
  note       = {arXiv:2209.14988 [cs, stat]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
  annote     = {Comment: see project page at https://dreamfusion3d.github.io/},
  file       = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/CMMT2IN9/Poole et al. - 2022 - DreamFusion Text-to-3D using 2D Diffusion.pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/YPRB3S6R/2209.html:text/html}
}

@misc{lin_magic3d_2023,
  title      = {{Magic3D}: {High}-{Resolution} {Text}-to-{3D} {Content} {Creation}},
  shorttitle = {{Magic3D}},
  url        = {http://arxiv.org/abs/2211.10440},
  doi        = {10.48550/arXiv.2211.10440},
  abstract   = {DreamFusion has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF), achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2x faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7\% raters to prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications.},
  urldate    = {2024-03-31},
  publisher  = {arXiv},
  author     = {Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},
  month      = mar,
  year       = {2023},
  note       = {arXiv:2211.10440 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
  annote     = {Comment: Accepted to CVPR 2023 as highlight. Project website: https://research.nvidia.com/labs/dir/magic3d},
  file       = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/533GFXY3/Lin et al. - 2023 - Magic3D High-Resolution Text-to-3D Content Creati.pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/6WIK8CJA/2211.html:text/html}
}

@inproceedings{kerr_lerf_2023,
  title      = {{LERF}: {Language} {Embedded} {Radiance} {Fields}},
  shorttitle = {{LERF}},
  url        = {https://openaccess.thecvf.com/content/ICCV2023/html/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.html?trk=public_post_comment-text},
  language   = {en},
  urldate    = {2024-03-31},
  author     = {Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
  year       = {2023},
  pages      = {19729--19739},
  file       = {Full Text PDF:/Users/felix/Zotero/storage/Q9X4DZMN/Kerr et al. - 2023 - LERF Language Embedded Radiance Fields.pdf:application/pdf}
}

@misc{wang_neus_2023,
  title      = {{NeuS}: {Learning} {Neural} {Implicit} {Surfaces} by {Volume} {Rendering} for {Multi}-view {Reconstruction}},
  shorttitle = {{NeuS}},
  url        = {http://arxiv.org/abs/2106.10689},
  doi        = {10.48550/arXiv.2106.10689},
  abstract   = {We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.},
  urldate    = {2024-03-31},
  publisher  = {arXiv},
  author     = {Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
  month      = feb,
  year       = {2023},
  note       = {arXiv:2106.10689 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
  annote     = {Comment: 23 pages},
  file       = {arXiv Fulltext PDF:/Users/felix/Zotero/storage/ZYY7TVYQ/Wang et al. - 2023 - NeuS Learning Neural Implicit Surfaces by Volume .pdf:application/pdf;arXiv.org Snapshot:/Users/felix/Zotero/storage/727AC6BW/2106.html:text/html}
}

@article{muller_instant_2022,
  title    = {Instant neural graphics primitives with a multiresolution hash encoding},
  volume   = {41},
  issn     = {0730-0301},
  url      = {https://dl.acm.org/doi/10.1145/3528223.3530127},
  doi      = {10.1145/3528223.3530127},
  abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
  number   = {4},
  urldate  = {2024-03-31},
  journal  = {ACM Transactions on Graphics},
  author   = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  month    = jul,
  year     = {2022},
  keywords = {encodings, function approximation, GPUs, hashing, image synthesis, neural networks, parallel computation},
  pages    = {102:1--102:15},
  file     = {Full Text PDF:/Users/felix/Zotero/storage/5ZIA5IMS/Müller et al. - 2022 - Instant neural graphics primitives with a multires.pdf:application/pdf}
}

@inproceedings{wang_3dn_2019,
  title      = {{3DN}: {3D} {Deformation} {Network}},
  shorttitle = {{3DN}},
  url        = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_3DN_3D_Deformation_Network_CVPR_2019_paper.html},
  urldate    = {2024-04-10},
  author     = {Wang, Weiyue and Ceylan, Duygu and Mech, Radomir and Neumann, Ulrich},
  year       = {2019},
  pages      = {1038--1046},
  file       = {Full Text PDF:/Users/felix/Zotero/storage/8BKI89FD/Wang et al. - 2019 - 3DN 3D Deformation Network.pdf:application/pdf}
}

@article{fu_geo-neus_2022,
  title      = {Geo-{Neus}: {Geometry}-{Consistent} {Neural} {Implicit} {Surfaces} {Learning} for {Multi}-view {Reconstruction}},
  volume     = {35},
  shorttitle = {Geo-{Neus}},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/16415eed5a0a121bfce79924db05d3fe-Abstract-Conference.html},
  language   = {en},
  urldate    = {2024-04-10},
  journal    = {Advances in Neural Information Processing Systems},
  author     = {Fu, Qiancheng and Xu, Qingshan and Ong, Yew Soon and Tao, Wenbing},
  month      = dec,
  year       = {2022},
  pages      = {3403--3416},
  file       = {Full Text PDF:/Users/felix/Zotero/storage/YYTBXCS5/Fu et al. - 2022 - Geo-Neus Geometry-Consistent Neural Implicit Surf.pdf:application/pdf}
}

@inproceedings{pan_deep_2019,
  title   = {Deep {Mesh} {Reconstruction} {From} {Single} {RGB} {Images} via {Topology} {Modification} {Networks}},
  url     = {https://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Deep_Mesh_Reconstruction_From_Single_RGB_Images_via_Topology_Modification_ICCV_2019_paper.html},
  urldate = {2024-04-11},
  author  = {Pan, Junyi and Han, Xiaoguang and Chen, Weikai and Tang, Jiapeng and Jia, Kui},
  year    = {2019},
  pages   = {9964--9973},
  file    = {Full Text PDF:/Users/felix/Zotero/storage/5U2EDZ3D/Pan et al. - 2019 - Deep Mesh Reconstruction From Single RGB Images vi.pdf:application/pdf}
}
